{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08739e3-8b40-44d1-90d9-cfb215b9e5f1",
   "metadata": {},
   "source": [
    "## Chain\n",
    "1. Take natural language query from\n",
    "2. Translate (using code LLM) the natural language query from #1 into SQL command\n",
    "3. Run the query from #2 against the db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372f028",
   "metadata": {},
   "source": [
    "#### Import project dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12180d6a-601f-4014-8795-06af63803d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# from langchain_community.utilities import SQLDatabase\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from functools import partial\n",
    "\n",
    "import langchain\n",
    "langchain.verbose=True\n",
    "\n",
    "import sqlglot\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019e56b",
   "metadata": {},
   "source": [
    "#### Pull database into project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c6e225-bb90-45a5-9bc3-b8bc291ae53a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE player_stats (\n",
      "\t\"index\" INTEGER, \n",
      "\tyear TEXT, \n",
      "\t\"Season_type\" TEXT, \n",
      "\t\"PLAYER_ID\" INTEGER, \n",
      "\t\"RANK\" INTEGER, \n",
      "\t\"PLAYER\" TEXT, \n",
      "\t\"TEAM_ID\" INTEGER, \n",
      "\t\"TEAM\" TEXT, \n",
      "\t\"GP\" INTEGER, \n",
      "\t\"MIN\" INTEGER, \n",
      "\t\"FGM\" INTEGER, \n",
      "\t\"FGA\" INTEGER, \n",
      "\t\"FG_PCT\" REAL, \n",
      "\t\"FG3M\" INTEGER, \n",
      "\t\"FG3A\" INTEGER, \n",
      "\t\"FG3_PCT\" REAL, \n",
      "\t\"FTM\" INTEGER, \n",
      "\t\"FTA\" INTEGER, \n",
      "\t\"FT_PCT\" REAL, \n",
      "\t\"OREB\" INTEGER, \n",
      "\t\"DREB\" INTEGER, \n",
      "\t\"REB\" INTEGER, \n",
      "\t\"AST\" INTEGER, \n",
      "\t\"STL\" INTEGER, \n",
      "\t\"BLK\" INTEGER, \n",
      "\t\"TOV\" INTEGER, \n",
      "\t\"PF\" INTEGER, \n",
      "\t\"PTS\" INTEGER, \n",
      "\t\"AST_TOV\" REAL, \n",
      "\t\"STL_TOV\" REAL\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from player_stats table:\n",
      "index\tyear\tSeason_type\tPLAYER_ID\tRANK\tPLAYER\tTEAM_ID\tTEAM\tGP\tMIN\tFGM\tFGA\tFG_PCT\tFG3M\tFG3A\tFG3_PCT\tFTM\tFTA\tFT_PCT\tOREB\tDREB\tREB\tAST\tSTL\tBLK\tTOV\tPF\tPTS\tAST_TOV\tSTL_TOV\n",
      "0\t2012-13\tRegular_Season\t201142\t1\tKevin Durant\t1610612760\tOKC\t81\t3119\t731\t1433\t0.51\t139\t334\t0.416\t679\t750\t0.905\t46\t594\t640\t374\t116\t105\t280\t143\t2280\t1.34\t0.41\n",
      "1\t2012-13\tRegular_Season\t977\t2\tKobe Bryant\t1610612747\tLAL\t78\t3013\t738\t1595\t0.463\t132\t407\t0.324\t525\t626\t0.839\t66\t367\t433\t469\t106\t25\t287\t173\t2133\t1.63\t0.37\n",
      "2\t2012-13\tRegular_Season\t2544\t3\tLeBron James\t1610612748\tMIA\t76\t2877\t765\t1354\t0.565\t103\t254\t0.406\t403\t535\t0.753\t97\t513\t610\t551\t129\t67\t226\t110\t2036\t2.44\t0.57\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "db=SQLDatabase.from_uri(\"sqlite:///nba_stats.db\")\n",
    "print(db.get_table_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad15ea",
   "metadata": {},
   "source": [
    "### Create functions that will be used as chain inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84b898-1756-4b2c-af8b-471a70e102d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_schema(_):\n",
    "    '''\n",
    "    Input: This function does not require a input '_'\n",
    "    Output: Database info about db tables used as schema\n",
    "    '''\n",
    "    return db.get_table_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0bdc89e-8684-4dea-9fa7-04b8cf69cb5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_sql(query: str, dialect: str) -> str:\n",
    "    \"\"\"\n",
    "    Transpile and execute an SQL query using the specified dialect.\n",
    "    Parameters:\n",
    "    - query (str): The SQL query to be transpiled and executed.\n",
    "    - dialect (str): The target SQL dialect for transpilation and execution.\n",
    "    Returns:\n",
    "    str: The result of executing the transpiled SQL query.\n",
    "    Example:\n",
    "    ```\n",
    "    result = run_sql(\"SELECT * FROM my_table\", \"sqlite\")\n",
    "    print(result)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    query=sqlglot.transpile(query, read=\"postgres\", write=dialect)[0]\n",
    "    print(query)\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414e270-f451-42a3-a24c-6c9f827b19bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "api_key = os.getenv(\"NV_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7720c0",
   "metadata": {},
   "source": [
    "### Create a instance of ChatNVIDIA model name -- `mistralai/mamba-codestral-7b-v0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6604d26-4490-4f58-b111-d3a597f3d349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_llm=ChatNVIDIA(\n",
    "    model=\"mistralai/mamba-codestral-7b-v0.1\",\n",
    "    max_tokens=500,\n",
    "    api_key = api_key\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d7bcb9",
   "metadata": {},
   "source": [
    "### Prepare SQL Prompt Template {input: question, schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e74885-f5ab-483d-9453-b46eadd14620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SQL_GEN_TEMPLATE=\"\"\"\n",
    "### Task\n",
    "Generate a SQLite query to answer, but use the LIMIT clause to limit the output to 5 results. Always use ILIKE on TEXT columns [QUESTION]{question}[/QUESTION]\n",
    "### Instructions\n",
    "- Always provide all the columns, If you cannot answer the question with the available database schema, return 'I do not know'\n",
    "### Database Schema\n",
    "The query will run on a database with the following schema:\n",
    "{schema}\n",
    "### Answer\n",
    "Given the database schema, here is the SQLite query that answers [QUESTION]{question}[/QUESTION]\n",
    "[SUFFIX][/SQL][PREFIX]‚ñÅ[SQL]\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d0082",
   "metadata": {},
   "source": [
    "### Create and invoke chain - save results in variable `response`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ca537-f3e4-4cf9-b5fe-3affe43ef3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(FG_PCT) as AVG_FG_PCT, FG_PCT as Kobe_FG_PCT, PLAYER\n",
      "FROM player_stats\n",
      "WHERE PLAYER LIKE '%Kobe Bryant%'\n",
      "LIMIT 5\n"
     ]
    }
   ],
   "source": [
    "prompt=PromptTemplate.from_template(SQL_GEN_TEMPLATE)\n",
    "sql_chain=(\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | sql_llm.bind(stop=[\"[/SQL]\", \";\"])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "response=sql_chain.invoke({\"question\": \"How does Kobe Bryant's shooting percentage compare to everyone's average shooting percentage?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d713f9e-858a-4157-b448-3d195eb5b0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(FG_PCT) AS AVG_FG_PCT, FG_PCT AS Kobe_FG_PCT, PLAYER FROM player_stats WHERE PLAYER LIKE '%Kobe Bryant%' LIMIT 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(0.40475000000000005, 0.463, 'Kobe Bryant')]\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_sql(response, 'sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f99e5",
   "metadata": {},
   "source": [
    "### Create a instance of ChatNVIDIA model name -- `meta/llama3-70b-instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f07d9e0-9d5b-49f6-994e-c8e6b1cbbb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:176: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chat_llm=ChatNVIDIA(\n",
    "    model = \"meta/llama3-70b-instruct\",\n",
    "    max_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9d40d",
   "metadata": {},
   "source": [
    "### Create a `SystemMessageTemplate` and a `HumanMessageTemplate` to use for the `ChatPromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae6fca26-bef3-4af1-a2e4-20e1632396ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SQL_SUM_SYS_MSG=(\n",
    "    \"Given an input question and SQL response, convert it to a natural language answer.\"\n",
    "    \" Give a human like response\"\n",
    ")\n",
    "SQL_SUM_TEMPLATE=\"\"\"\n",
    "Based on the table schema below, question, sql query, and sql response,\\\n",
    " write a natural language response. Be generic and avoid errors as much as possible.\n",
    "{schema}\n",
    "Question: {question}\n",
    "SQL Query: {sqlquery}\n",
    "SQL Response: {response}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73017d6e",
   "metadata": {},
   "source": [
    "### Create a new chain `full_chain`, passing the previous outcome from `sql_chain` in as the first input\n",
    "This is a crucial step where the model uses a RunnablePassthrough.assign tool to pass through the response from the sql_chain as the first input to the full_chain. \n",
    "This is where the natural language input is converted to SQL code to retrieve relevant information and return the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92526722-8876-415c-9920-768fd4d98042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_response=ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SQL_SUM_SYS_MSG),\n",
    "    (\"human\", SQL_SUM_TEMPLATE),\n",
    "])\n",
    "history=[]\n",
    "def save_return(d, history=[]):\n",
    "    history+=[d]\n",
    "    return d\n",
    "full_chain=(\n",
    "    RunnablePassthrough.assign(sqlquery=sql_chain)\n",
    "    | RunnablePassthrough.assign(\n",
    "        schema=get_schema,\n",
    "        response=lambda x: run_sql(x[\"sqlquery\"], \"sqlite\") or \"ERROR RETRIEVING FROM DATABASE\",\n",
    "    )\n",
    "    | prompt_response\n",
    "    | partial(save_return, history=history)\n",
    "    | chat_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6353b04-e82f-4679-979f-9924642dfaae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"PLAYER\", \"FG_PCT\" AS \"Shooting_Pct\"\n",
      "FROM player_stats\n",
      "WHERE \"FG_PCT\" < 0.5\n",
      "LIMIT 5\n"
     ]
    }
   ],
   "source": [
    "print(sql_response.invoke({'question': 'how do i improve my shooting percentage?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96f5b044-5f15-4e67-999e-d9bff76eda7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"PLAYER\", \"FG_PCT\" AS \"Shooting_Pct\" FROM player_stats WHERE \"FG_PCT\" < 0.5 LIMIT 5\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[401] Unauthorized\nInvalid JWT serialization: Missing dot delimiter(s)\nPlease check or regenerate your API key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(full_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow do i improve my shooting percentage?\u001b[39m\u001b[38;5;124m'\u001b[39m}))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:382\u001b[0m, in \u001b[0;36mChatNVIDIA._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m inputs, extra_headers \u001b[38;5;241m=\u001b[39m _process_for_vlm(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    381\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_payload(inputs\u001b[38;5;241m=\u001b[39minputs, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_req(payload\u001b[38;5;241m=\u001b[39mpayload, extra_headers\u001b[38;5;241m=\u001b[39mextra_headers)\n\u001b[1;32m    383\u001b[0m responses, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpostprocess(response)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_callback_out(responses, run_manager)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:473\u001b[0m, in \u001b[0;36m_NVIDIAClient.get_req\u001b[0;34m(self, payload, extra_headers)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_req\u001b[39m(\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    469\u001b[0m     payload: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    470\u001b[0m     extra_headers: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    471\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post to the API.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     response, session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_url, payload, extra_headers\u001b[38;5;241m=\u001b[39mextra_headers\n\u001b[1;32m    475\u001b[0m     )\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(response, session)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:369\u001b[0m, in \u001b[0;36m_NVIDIAClient._post\u001b[0;34m(self, invoke_url, payload, extra_headers)\u001b[0m\n\u001b[1;32m    365\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_fn()\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add_authorization(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_inputs)\n\u001b[1;32m    368\u001b[0m )\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_raise(response)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, session\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:462\u001b[0m, in \u001b[0;36m_NVIDIAClient._try_raise\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    460\u001b[0m     body \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease check or regenerate your API key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# todo: raise as an HTTPError\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: [401] Unauthorized\nInvalid JWT serialization: Missing dot delimiter(s)\nPlease check or regenerate your API key."
     ]
    }
   ],
   "source": [
    "print(full_chain.invoke({'question': 'how do i improve my shooting percentage?'}))\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ca40d-1b05-4135-9e0f-ee1f21b50d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_chain.invoke({'question': 'who is the best 3 point shooter?'}))\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48094d46",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (238439787.py, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 36\u001b[0;36m\u001b[0m\n\u001b[0;31m    submit_button.click(process_query, inputs=nl_query_input, outputs=[sql &#8203;:contentReference[oaicite:0]{index=0}&#8203;\u001b[0m\n\u001b[0m                                                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.chat_models import ChatOpenAI  # Replace this with your Llama integration\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Function to process natural language query\n",
    "def process_query(nl_query):\n",
    "    try:\n",
    "        # Step 1: Translate natural language query into SQL\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Translate the following natural language query into SQL:\\n\\n{query}\",\n",
    "        )\n",
    "        llm = ChatOpenAI(temperature=0)  # Replace with your Llama LLM initialization\n",
    "        sql_query = llm.generate(prompt_template.format(query=nl_query))\n",
    "        \n",
    "        # Step 2: Execute SQL query on the database\n",
    "        db = SQLDatabase.from_uri(\"sqlite:///example.db\")  # Update with your database connection string\n",
    "        results = db.run(sql_query)\n",
    "        return sql_query, results\n",
    "    except Exception as e:\n",
    "        return f\"Error generating SQL: {str(e)}\", \"No results\"\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Natural Language to SQL Query with Llama LLM\")\n",
    "    nl_query_input = gr.Textbox(\n",
    "        label=\"Enter your natural language query\",\n",
    "        placeholder=\"e.g., Show me all users who joined last month.\",\n",
    "        lines=2,\n",
    "    )\n",
    "    sql_output = gr.Textbox(label=\"Generated SQL Query\", lines=2, interactive=False)\n",
    "    query_results = gr.Textbox(label=\"Query Results\", lines=5, interactive=False)\n",
    "    submit_button = gr.Button(\"Run Query\")\n",
    "    submit_button.click(process_query, inputs=nl_query_input, outputs=[sql &#8203;:contentReference[oaicite:0]{index=0}&#8203;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db94d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e5e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f20abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b435cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b375d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f0555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb462db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ea56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917147c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
